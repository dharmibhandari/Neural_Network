import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.metrics import BinaryAccuracy

df = pd.read_csv(r'/content/sample_data/creadit_card.csv')


df.head()

df.dropna()

df = df[df['age'] >= 0]

print(df.columns)


target = df['c#default']


scaler = StandardScaler()
features_scaled = scaler.fit_transform(features)

train_features, test_features, train_target, test_target = train_test_split(features_scaled, target, test_size=0.2, random_state=42)

def build_model():
    model = Sequential()
    model.add(Dense(10, input_dim=train_features.shape[1], activation='relu'))
    model.add(Dense(6, activation='relu'))
    model.add(Dense(3, activation='relu'))
    model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification
    return model

model = build_model()
model.compile(optimizer=Adam(learning_rate=0.001),
              loss='binary_crossentropy',
              metrics=['accuracy'])

history = model.fit(train_features, train_target, epochs=30, batch_size=10, validation_data=(test_features, test_target))



history = model.fit(train_features, train_target, epochs=30, batch_size=10, validation_data=(test_features, test_target))

predictions = model.predict(test_features)
predictions = (predictions > 0.5).astype(int)  # Convert probabilities to binary (0 or 1)
print("Predictions on the test set:")
print(predictions[:10])

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# Load your data (adjust the column name for the target variable as needed)
# Assuming 'c#default' is the target
df = pd.read_csv(r'/content/sample_data/creadit_card.csv')

# Separate features and target
X = df[['income', 'age', 'loan']]
y = df['c#default']

# Split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Create a neural network model
model = Sequential()
model.add(Dense(16, input_dim=3, activation='relu'))  # Input layer
model.add(Dense(8, activation='relu'))               # Hidden layer
model.add(Dense(1, activation='sigmoid'))            # Output layer

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train, epochs=100, batch_size=10, validation_split=0.2)

# Make predictions
predictions = model.predict(X_test)
predictions = (predictions > 0.5).astype(int)

# Display predictions
print("Predictions on the test set:")
print(predictions[:10])
